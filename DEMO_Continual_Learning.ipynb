{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ixMdF3VXe_TT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Buffer"
      ],
      "metadata": {
        "id": "8NGoi8HHTYgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self,size):\n",
        "        self.size=size\n",
        "        self.buffer=[]\n",
        "    def addSamples(self,X,y):\n",
        "        if len(self.buffer)>=self.size:\n",
        "            self.buffer=self.buffer[len(X):]\n",
        "        self.buffer.extend(list(zip(X,y)))\n",
        "    def getSamples(self):\n",
        "        if len(self.buffer)==0:\n",
        "            return None,None\n",
        "        X,y=zip(*self.buffer)\n",
        "        return torch.stack(X),torch.stack(y)"
      ],
      "metadata": {
        "id": "jMhyL8zITcgf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HAT CNN"
      ],
      "metadata": {
        "id": "wVN17AknTgNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HATCNN(nn.Module):\n",
        "    def __init__(self,numTasks):\n",
        "        super(HATCNN,self).__init__()\n",
        "        self.num_tasks=numTasks\n",
        "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1)\n",
        "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1)\n",
        "        self.pool=nn.MaxPool2d(2,2)\n",
        "        self.fc1=nn.Linear(64*8*8,128)\n",
        "        self.fc2=nn.Linear(128,2)\n",
        "        self.masks = nn.Parameter(torch.ones(numTasks,128)) #Masking per Tasks\n",
        "    def forward(self,x,task_id):\n",
        "        x=self.pool(torch.relu(self.conv1(x)))#(T,32,16,16)\n",
        "        x=self.pool(torch.relu(self.conv2(x))) #(T,64,8,8)\n",
        "        x=x.view(x.size(0),-1) #(T,4096)\n",
        "        mask=self.masks[task_id]\n",
        "        x=torch.relu(self.fc1(x)*mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TM56EysPThil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WP CNN"
      ],
      "metadata": {
        "id": "AgScjTCuTlnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiTaskCNN, self).__init__()\n",
        "        self.task_heads = nn.ModuleList([\n",
        "            nn.Linear(128,2),  #Task 0\n",
        "            nn.Linear(128,2), #Task 1\n",
        "            nn.Linear(128,2),  #Task 2\n",
        "            nn.Linear(128,2),  #Task 3\n",
        "            nn.Linear(128,2)])   #Task 4\n",
        "    def forward(self, x, task_id):\n",
        "        return self.task_heads[task_id](x)"
      ],
      "metadata": {
        "id": "U16Mg-dlaDFw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OOD CNN"
      ],
      "metadata": {
        "id": "2Lh1fqPZTrTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OODCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OODCNN, self).__init__()\n",
        "        self.task_heads = nn.ModuleList([\n",
        "            nn.Linear(128,2),  #Task 0\n",
        "            nn.Linear(128,2), #Task 1\n",
        "            nn.Linear(128,2),  #Task 2\n",
        "            nn.Linear(128,2),  #Task 3\n",
        "            nn.Linear(128,2)])   #Task 4\n",
        "    def forward(self, x, task_id):\n",
        "        return self.task_heads[task_id](x)"
      ],
      "metadata": {
        "id": "VQaGJWwmTtBM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING OODs"
      ],
      "metadata": {
        "id": "iLYDR98aqnFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def loadTaskDataset(trainData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(trainData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(trainData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=True)\n",
        "\n",
        "def traingOODheadWithFeatures(trainData,taskClasses,oodModel,featureExtractor,epochs=10):\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(list(featureExtractor.parameters())+list(oodModel.parameters()),lr=0.001)\n",
        "    for taskNum,taskClasses in enumerate(classGroups):\n",
        "        print(f\"\\nTraining on task {taskNum+1} with classes: {taskClasses}\")\n",
        "        taskLoader=loadTaskDataset(trainDataset,taskClasses)\n",
        "        XTaskList,yTaskList=[],[]\n",
        "        for images,labels in taskLoader:\n",
        "            XTaskList.append(images)\n",
        "            yTaskList.append(labels)\n",
        "        XTask=torch.cat(XTaskList)\n",
        "        yTask=torch.cat(yTaskList)\n",
        "        for epoch in range(epochs):\n",
        "            featureExtractor.train()\n",
        "            oodModel.train()\n",
        "            optimizer.zero_grad()\n",
        "            features=featureExtractor(XTask,taskNum)\n",
        "            outputs=oodModel(features,taskNum)\n",
        "            loss=criterion(outputs,yTask)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Task {taskNum+1}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "trainDataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "featureExtractor=HATCNN(numTasks=len(classGroups))\n",
        "oodModel=OODCNN()\n",
        "traingOODheadWithFeatures(trainDataset,classGroups,oodModel,featureExtractor,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcQBh8FFqrfY",
        "outputId": "d8753720-af36-41e8-f537-7c12d25bb59c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Training on task 1 with classes: [0, 1]\n",
            "Task 1, Epoch 1, Loss: 0.6945529580116272\n",
            "Task 1, Epoch 2, Loss: 0.651195764541626\n",
            "Task 1, Epoch 3, Loss: 0.652748167514801\n",
            "Task 1, Epoch 4, Loss: 0.5800525546073914\n",
            "Task 1, Epoch 5, Loss: 0.5939843654632568\n",
            "Task 1, Epoch 6, Loss: 0.5417751669883728\n",
            "Task 1, Epoch 7, Loss: 0.5321953296661377\n",
            "Task 1, Epoch 8, Loss: 0.5126423239707947\n",
            "Task 1, Epoch 9, Loss: 0.47330132126808167\n",
            "Task 1, Epoch 10, Loss: 0.4722510278224945\n",
            "\n",
            "Training on task 2 with classes: [2, 3]\n",
            "Task 2, Epoch 1, Loss: 0.7303708791732788\n",
            "Task 2, Epoch 2, Loss: 0.69655442237854\n",
            "Task 2, Epoch 3, Loss: 0.6973191499710083\n",
            "Task 2, Epoch 4, Loss: 0.6879715323448181\n",
            "Task 2, Epoch 5, Loss: 0.6641501784324646\n",
            "Task 2, Epoch 6, Loss: 0.6574621796607971\n",
            "Task 2, Epoch 7, Loss: 0.6581306457519531\n",
            "Task 2, Epoch 8, Loss: 0.6403805017471313\n",
            "Task 2, Epoch 9, Loss: 0.6229217052459717\n",
            "Task 2, Epoch 10, Loss: 0.6215378642082214\n",
            "\n",
            "Training on task 3 with classes: [4, 5]\n",
            "Task 3, Epoch 1, Loss: 0.7039634585380554\n",
            "Task 3, Epoch 2, Loss: 0.6801877617835999\n",
            "Task 3, Epoch 3, Loss: 0.6742595434188843\n",
            "Task 3, Epoch 4, Loss: 0.6465389132499695\n",
            "Task 3, Epoch 5, Loss: 0.6285971999168396\n",
            "Task 3, Epoch 6, Loss: 0.6194588541984558\n",
            "Task 3, Epoch 7, Loss: 0.5966325402259827\n",
            "Task 3, Epoch 8, Loss: 0.588153064250946\n",
            "Task 3, Epoch 9, Loss: 0.5718582272529602\n",
            "Task 3, Epoch 10, Loss: 0.5549396872520447\n",
            "\n",
            "Training on task 4 with classes: [6, 7]\n",
            "Task 4, Epoch 1, Loss: 0.7045178413391113\n",
            "Task 4, Epoch 2, Loss: 0.6700564026832581\n",
            "Task 4, Epoch 3, Loss: 0.6395619511604309\n",
            "Task 4, Epoch 4, Loss: 0.601387619972229\n",
            "Task 4, Epoch 5, Loss: 0.5704611539840698\n",
            "Task 4, Epoch 6, Loss: 0.5492274165153503\n",
            "Task 4, Epoch 7, Loss: 0.5215418934822083\n",
            "Task 4, Epoch 8, Loss: 0.5031402111053467\n",
            "Task 4, Epoch 9, Loss: 0.47895073890686035\n",
            "Task 4, Epoch 10, Loss: 0.4569976031780243\n",
            "\n",
            "Training on task 5 with classes: [8, 9]\n",
            "Task 5, Epoch 1, Loss: 0.7672423124313354\n",
            "Task 5, Epoch 2, Loss: 0.7019235491752625\n",
            "Task 5, Epoch 3, Loss: 0.6473707556724548\n",
            "Task 5, Epoch 4, Loss: 0.5912358164787292\n",
            "Task 5, Epoch 5, Loss: 0.5697838068008423\n",
            "Task 5, Epoch 6, Loss: 0.5306934118270874\n",
            "Task 5, Epoch 7, Loss: 0.523149847984314\n",
            "Task 5, Epoch 8, Loss: 0.49558931589126587\n",
            "Task 5, Epoch 9, Loss: 0.4938385784626007\n",
            "Task 5, Epoch 10, Loss: 0.47389423847198486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "0l3Oxcr1T_7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTaskDataset(trainData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(trainData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(trainData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=True)\n",
        "\n",
        "def evaluateTaskAccuracy(model,featureExtractor,taskLoader,taskNum):\n",
        "    model.eval()\n",
        "    correct,total=0,0\n",
        "    with torch.no_grad():\n",
        "        for images,labels in taskLoader:\n",
        "            features=featureExtractor(images,taskNum)\n",
        "            outputs=model(features,taskNum).argmax(dim=1)\n",
        "            correct+=(outputs==labels).sum().item()\n",
        "            total+=labels.size(0)\n",
        "    return correct/total if total>0 else 0\n",
        "\n",
        "def trainIncrementalClassTasks(featureExtractor,wpModel,trainDataset,classGroups,epochs=10):\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(list(wpModel.parameters()),lr=0.001)\n",
        "    buffer=ReplayBuffer(size=1000)\n",
        "    task_accuracies,avg_forgetting_rate=[],[]\n",
        "    for taskNum,taskClasses in enumerate(classGroups):\n",
        "        print(f\"\\nTraining on task {taskNum+1} with classes: {taskClasses}\")\n",
        "        taskLoader=loadTaskDataset(trainDataset,taskClasses)\n",
        "        XTaskList,yTaskList=[],[]\n",
        "        for images,labels in taskLoader:\n",
        "            XTaskList.append(images)\n",
        "            yTaskList.append(labels)\n",
        "        XTask=torch.cat(XTaskList)\n",
        "        yTask=torch.cat(yTaskList)\n",
        "        XBuffer,yBuffer=buffer.getSamples()\n",
        "        if XBuffer is not None and len(XBuffer)>0:\n",
        "            XReplay,yReplay=XBuffer,yBuffer\n",
        "        else:\n",
        "            XReplay,yReplay=None,None\n",
        "        for epoch in range(epochs):\n",
        "            featureExtractor.train()\n",
        "            wpModel.train()\n",
        "            optimizer.zero_grad()\n",
        "            features=featureExtractor(XTask,taskNum)\n",
        "            outputs=wpModel(features,taskNum)\n",
        "            loss=criterion(outputs,yTask)\n",
        "            if XReplay is not None:\n",
        "                features=featureExtractor(XReplay,taskNum)\n",
        "                replayOutputs=wpModel(features,taskNum)\n",
        "                replayLoss=criterion(replayOutputs,yReplay)\n",
        "                loss+=replayLoss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Task {taskNum+1}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "        task_accuracy=evaluateTaskAccuracy(wpModel,featureExtractor,taskLoader,taskNum)\n",
        "        print(f\"Accuracy on task {taskNum+1}: {task_accuracy*100:.2f}%\")\n",
        "        task_accuracies.append([task_accuracy])\n",
        "        for i in range(taskNum):\n",
        "            previous_task_loader=loadTaskDataset(trainDataset,classGroups[i])\n",
        "            accuracy_after_task=evaluateTaskAccuracy(wpModel,featureExtractor,previous_task_loader,taskNum)\n",
        "            task_accuracies[i].append(accuracy_after_task)\n",
        "            print(f\"Accuracy on task {i+1} after learning task {taskNum+1}: {accuracy_after_task*100:.2f}%\")\n",
        "        if taskNum>0:\n",
        "            forgetting_rates=[]\n",
        "            for i in range(taskNum):\n",
        "                initial_accuracy=task_accuracies[i][0]\n",
        "                current_accuracy=task_accuracies[i][-1]\n",
        "                forgetting=initial_accuracy-current_accuracy\n",
        "                forgetting_rates.append(forgetting)\n",
        "            avg_forgetting_rate.append(sum(forgetting_rates)/len(forgetting_rates))\n",
        "            print(f\"Average Forgetting Rate after task {taskNum+1}: {avg_forgetting_rate[-1]:.4f}\")\n",
        "        buffer.addSamples(XTask,yTask)\n",
        "    aca=sum([task_accuracies[i][-1] for i in range(len(classGroups))])/len(classGroups)\n",
        "    print(f\"\\nAverage Classification Accuracy (ACA) after the last task: {aca:.4f}\")\n",
        "    return aca,avg_forgetting_rate\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "trainDataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "wpModel=MultiTaskCNN()\n",
        "trainIncrementalClassTasks(featureExtractor,wpModel,trainDataset,classGroups)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M2qQd0ywIbM",
        "outputId": "49a7d387-4a40-4bde-d633-dc3029affd3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Training on task 1 with classes: [0, 1]\n",
            "Task 1, Epoch 1, Loss: 1.0068581104278564\n",
            "Task 1, Epoch 2, Loss: 0.9093834161758423\n",
            "Task 1, Epoch 3, Loss: 0.8244733214378357\n",
            "Task 1, Epoch 4, Loss: 0.7548810243606567\n",
            "Task 1, Epoch 5, Loss: 0.7029379606246948\n",
            "Task 1, Epoch 6, Loss: 0.6696040034294128\n",
            "Task 1, Epoch 7, Loss: 0.6534898281097412\n",
            "Task 1, Epoch 8, Loss: 0.65055912733078\n",
            "Task 1, Epoch 9, Loss: 0.655038058757782\n",
            "Task 1, Epoch 10, Loss: 0.6612259745597839\n",
            "Accuracy on task 1: 57.98%\n",
            "\n",
            "Training on task 2 with classes: [2, 3]\n",
            "Task 2, Epoch 1, Loss: 2.2999584674835205\n",
            "Task 2, Epoch 2, Loss: 2.1008553504943848\n",
            "Task 2, Epoch 3, Loss: 1.9156476259231567\n",
            "Task 2, Epoch 4, Loss: 1.747509241104126\n",
            "Task 2, Epoch 5, Loss: 1.5999573469161987\n",
            "Task 2, Epoch 6, Loss: 1.4765948057174683\n",
            "Task 2, Epoch 7, Loss: 1.380618929862976\n",
            "Task 2, Epoch 8, Loss: 1.3140560388565063\n",
            "Task 2, Epoch 9, Loss: 1.2767927646636963\n",
            "Task 2, Epoch 10, Loss: 1.2657253742218018\n",
            "Accuracy on task 2: 64.22%\n",
            "Accuracy on task 1 after learning task 2: 63.61%\n",
            "Average Forgetting Rate after task 2: -0.0563\n",
            "\n",
            "Training on task 3 with classes: [4, 5]\n",
            "Task 3, Epoch 1, Loss: 2.574169874191284\n",
            "Task 3, Epoch 2, Loss: 2.3589415550231934\n",
            "Task 3, Epoch 3, Loss: 2.1588001251220703\n",
            "Task 3, Epoch 4, Loss: 1.9770656824111938\n",
            "Task 3, Epoch 5, Loss: 1.8172707557678223\n",
            "Task 3, Epoch 6, Loss: 1.682802438735962\n",
            "Task 3, Epoch 7, Loss: 1.5762856006622314\n",
            "Task 3, Epoch 8, Loss: 1.4988703727722168\n",
            "Task 3, Epoch 9, Loss: 1.4496324062347412\n",
            "Task 3, Epoch 10, Loss: 1.4253355264663696\n",
            "Accuracy on task 3: 54.81%\n",
            "Accuracy on task 1 after learning task 3: 35.49%\n",
            "Accuracy on task 2 after learning task 3: 50.13%\n",
            "Average Forgetting Rate after task 3: 0.1829\n",
            "\n",
            "Training on task 4 with classes: [6, 7]\n",
            "Task 4, Epoch 1, Loss: 1.2460213899612427\n",
            "Task 4, Epoch 2, Loss: 1.2285361289978027\n",
            "Task 4, Epoch 3, Loss: 1.2177107334136963\n",
            "Task 4, Epoch 4, Loss: 1.2061805725097656\n",
            "Task 4, Epoch 5, Loss: 1.1942191123962402\n",
            "Task 4, Epoch 6, Loss: 1.1831409931182861\n",
            "Task 4, Epoch 7, Loss: 1.1736423969268799\n",
            "Task 4, Epoch 8, Loss: 1.1655648946762085\n",
            "Task 4, Epoch 9, Loss: 1.1583473682403564\n",
            "Task 4, Epoch 10, Loss: 1.1516274213790894\n",
            "Accuracy on task 4: 79.33%\n",
            "Accuracy on task 1 after learning task 4: 44.86%\n",
            "Accuracy on task 2 after learning task 4: 54.21%\n",
            "Accuracy on task 3 after learning task 4: 61.99%\n",
            "Average Forgetting Rate after task 4: 0.0532\n",
            "\n",
            "Training on task 5 with classes: [8, 9]\n",
            "Task 5, Epoch 1, Loss: 1.5181608200073242\n",
            "Task 5, Epoch 2, Loss: 1.460839033126831\n",
            "Task 5, Epoch 3, Loss: 1.4374561309814453\n",
            "Task 5, Epoch 4, Loss: 1.4350534677505493\n",
            "Task 5, Epoch 5, Loss: 1.4359803199768066\n",
            "Task 5, Epoch 6, Loss: 1.4309508800506592\n",
            "Task 5, Epoch 7, Loss: 1.4192583560943604\n",
            "Task 5, Epoch 8, Loss: 1.4039989709854126\n",
            "Task 5, Epoch 9, Loss: 1.388738751411438\n",
            "Task 5, Epoch 10, Loss: 1.375934362411499\n",
            "Accuracy on task 5: 49.36%\n",
            "Accuracy on task 1 after learning task 5: 49.33%\n",
            "Accuracy on task 2 after learning task 5: 64.94%\n",
            "Accuracy on task 3 after learning task 5: 62.82%\n",
            "Accuracy on task 4 after learning task 5: 63.68%\n",
            "Average Forgetting Rate after task 5: 0.0389\n",
            "\n",
            "Average Classification Accuracy (ACA) after the last task: 0.5803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.58026, [-0.05630000000000002, 0.1829, 0.05316666666666666, 0.038925])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "5Hm7c-_UN-k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "GG_dXwVy7jSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "def loadTaskTestDataset(testData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(testData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(testData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=False)\n",
        "\n",
        "def testModel(featureExtractor,wpModel,testDataset,classGroups):\n",
        "    wpModel.eval()\n",
        "    totalCorrect=0\n",
        "    totalSamples=0\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for taskNum,taskClasses in enumerate(classGroups):\n",
        "            print(f\"\\nTesting on task {taskNum+1} with classes: {taskClasses}\")\n",
        "            taskTestLoader=loadTaskTestDataset(testDataset,taskClasses)\n",
        "            taskCorrect=0\n",
        "            taskTotal=0\n",
        "            taskLoss=0\n",
        "            for images,labels in taskTestLoader:\n",
        "                features=featureExtractor(images,taskNum)\n",
        "                outputs=wpModel(features,taskNum)\n",
        "                loss=criterion(outputs,labels)\n",
        "                taskLoss+=loss.item()\n",
        "                _,predicted=torch.max(outputs,1)\n",
        "                taskTotal+=labels.size(0)\n",
        "                taskCorrect+=(predicted==labels).sum().item()\n",
        "            taskAccuracy=100*taskCorrect/taskTotal\n",
        "            avgLoss=taskLoss/len(taskTestLoader)\n",
        "            print(f\"Task {taskNum+1} Accuracy: {taskAccuracy:.2f}%\")\n",
        "            print(f\"Task {taskNum+1} Loss: {avgLoss:.4f}\")\n",
        "            totalCorrect+=taskCorrect\n",
        "            totalSamples+=taskTotal\n",
        "    overallAccuracy=100*totalCorrect/totalSamples\n",
        "    print(f\"\\nOverall Accuracy on all tasks: {overallAccuracy:.2f}%\")\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "testModel(featureExtractor,wpModel,testDataset,classGroups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx5cgNuSVpV5",
        "outputId": "f16c2bd7-6f2f-42f6-ec43-b28e92f1ac82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "\n",
            "Testing on task 1 with classes: [0, 1]\n",
            "Task 1 Accuracy: 57.45%\n",
            "Task 1 Loss: 0.6649\n",
            "\n",
            "Testing on task 2 with classes: [2, 3]\n",
            "Task 2 Accuracy: 63.00%\n",
            "Task 2 Loss: 0.6406\n",
            "\n",
            "Testing on task 3 with classes: [4, 5]\n",
            "Task 3 Accuracy: 54.25%\n",
            "Task 3 Loss: 0.6818\n",
            "\n",
            "Testing on task 4 with classes: [6, 7]\n",
            "Task 4 Accuracy: 81.20%\n",
            "Task 4 Loss: 0.4649\n",
            "\n",
            "Testing on task 5 with classes: [8, 9]\n",
            "Task 5 Accuracy: 50.90%\n",
            "Task 5 Loss: 0.7129\n",
            "\n",
            "Overall Accuracy on all tasks: 61.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FULL CODE"
      ],
      "metadata": {
        "id": "dIzr2FwyN6Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self,size):\n",
        "        self.size=size\n",
        "        self.buffer=[]\n",
        "    def addSamples(self,X,y):\n",
        "        if len(self.buffer)>=self.size:\n",
        "            self.buffer=self.buffer[len(X):]\n",
        "        self.buffer.extend(list(zip(X,y)))\n",
        "    def getSamples(self):\n",
        "        if len(self.buffer)==0:\n",
        "            return None,None\n",
        "        X,y=zip(*self.buffer)\n",
        "        return torch.stack(X),torch.stack(y)\n",
        "\n",
        "class HATCNN(nn.Module):\n",
        "    def __init__(self,numTasks):\n",
        "        super(HATCNN,self).__init__()\n",
        "        self.num_tasks=numTasks\n",
        "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1)\n",
        "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1)\n",
        "        self.pool=nn.MaxPool2d(2,2)\n",
        "        self.fc1=nn.Linear(64*8*8,128)\n",
        "        self.fc2=nn.Linear(128,2)\n",
        "        self.masks = nn.Parameter(torch.ones(numTasks,128)) #Masking per Tasks\n",
        "    def forward(self,x,task_id):\n",
        "        x=self.pool(torch.relu(self.conv1(x)))#(T,32,16,16)\n",
        "        x=self.pool(torch.relu(self.conv2(x))) #(T,64,8,8)\n",
        "        x=x.view(x.size(0),-1) #(T,4096)\n",
        "        mask=self.masks[task_id]\n",
        "        x=torch.relu(self.fc1(x)*mask)\n",
        "        return x\n",
        "\n",
        "class MultiTaskCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiTaskCNN, self).__init__()\n",
        "        self.task_heads = nn.ModuleList([\n",
        "            nn.Linear(128,2),  #Task 0\n",
        "            nn.Linear(128,2), #Task 1\n",
        "            nn.Linear(128,2),  #Task 2\n",
        "            nn.Linear(128,2),  #Task 3\n",
        "            nn.Linear(128,2)])   #Task 4\n",
        "    def forward(self, x, task_id):\n",
        "        return self.task_heads[task_id](x)\n",
        "\n",
        "class OODCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OODCNN, self).__init__()\n",
        "        self.task_heads = nn.ModuleList([\n",
        "            nn.Linear(128,2),  #Task 0\n",
        "            nn.Linear(128,2), #Task 1\n",
        "            nn.Linear(128,2),  #Task 2\n",
        "            nn.Linear(128,2),  #Task 3\n",
        "            nn.Linear(128,2)])   #Task 4\n",
        "    def forward(self, x, task_id):\n",
        "        return self.task_heads[task_id](x)\n",
        "\n",
        "\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def loadTaskDataset(trainData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(trainData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(trainData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=True)\n",
        "\n",
        "def traingOODheadWithFeatures(trainData,taskClasses,oodModel,featureExtractor,epochs=10):\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(list(featureExtractor.parameters())+list(oodModel.parameters()),lr=0.001)\n",
        "    for taskNum,taskClasses in enumerate(classGroups):\n",
        "        print(f\"\\nTraining on task {taskNum+1} with classes: {taskClasses}\")\n",
        "        taskLoader=loadTaskDataset(trainDataset,taskClasses)\n",
        "        XTaskList,yTaskList=[],[]\n",
        "        for images,labels in taskLoader:\n",
        "            XTaskList.append(images)\n",
        "            yTaskList.append(labels)\n",
        "        XTask=torch.cat(XTaskList)\n",
        "        yTask=torch.cat(yTaskList)\n",
        "        for epoch in range(epochs):\n",
        "            featureExtractor.train()\n",
        "            oodModel.train()\n",
        "            optimizer.zero_grad()\n",
        "            features=featureExtractor(XTask,taskNum)\n",
        "            outputs=oodModel(features,taskNum)\n",
        "            loss=criterion(outputs,yTask)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Task {taskNum+1}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "trainDataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "featureExtractor=HATCNN(numTasks=len(classGroups))\n",
        "oodModel=OODCNN()\n",
        "traingOODheadWithFeatures(trainDataset,classGroups,oodModel,featureExtractor,epochs=1)\n",
        "\n",
        "\n",
        "def loadTaskDataset(trainData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(trainData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(trainData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=True)\n",
        "\n",
        "def evaluateTaskAccuracy(model,featureExtractor,taskLoader,taskNum):\n",
        "    model.eval()\n",
        "    correct,total=0,0\n",
        "    with torch.no_grad():\n",
        "        for images,labels in taskLoader:\n",
        "            features=featureExtractor(images,taskNum)\n",
        "            outputs=model(features,taskNum).argmax(dim=1)\n",
        "            correct+=(outputs==labels).sum().item()\n",
        "            total+=labels.size(0)\n",
        "    return correct/total if total>0 else 0\n",
        "\n",
        "def trainIncrementalClassTasks(featureExtractor,wpModel,trainDataset,classGroups,epochs=5):\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    optimizer=optim.Adam(list(wpModel.parameters()),lr=0.001)\n",
        "    buffer=ReplayBuffer(size=1000)\n",
        "    task_accuracies,avg_forgetting_rate=[],[]\n",
        "    for taskNum,taskClasses in enumerate(classGroups):\n",
        "        print(f\"\\nTraining on task {taskNum+1} with classes: {taskClasses}\")\n",
        "        taskLoader=loadTaskDataset(trainDataset,taskClasses)\n",
        "        XTaskList,yTaskList=[],[]\n",
        "        for images,labels in taskLoader:\n",
        "            XTaskList.append(images)\n",
        "            yTaskList.append(labels)\n",
        "        XTask=torch.cat(XTaskList)\n",
        "        yTask=torch.cat(yTaskList)\n",
        "        XBuffer,yBuffer=buffer.getSamples()\n",
        "        if XBuffer is not None and len(XBuffer)>0:\n",
        "            XReplay,yReplay=XBuffer,yBuffer\n",
        "        else:\n",
        "            XReplay,yReplay=None,None\n",
        "        for epoch in range(epochs):\n",
        "            featureExtractor.train()\n",
        "            wpModel.train()\n",
        "            optimizer.zero_grad()\n",
        "            features=featureExtractor(XTask,taskNum)\n",
        "            outputs=wpModel(features,taskNum)\n",
        "            loss=criterion(outputs,yTask)\n",
        "            if XReplay is not None:\n",
        "                features=featureExtractor(XReplay,taskNum)\n",
        "                replayOutputs=wpModel(features,taskNum)\n",
        "                replayLoss=criterion(replayOutputs,yReplay)\n",
        "                loss+=replayLoss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Task {taskNum+1}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "        task_accuracy=evaluateTaskAccuracy(wpModel,featureExtractor,taskLoader,taskNum)\n",
        "        print(f\"Accuracy on task {taskNum+1}: {task_accuracy*100:.2f}%\")\n",
        "        task_accuracies.append([task_accuracy])\n",
        "        for i in range(taskNum):\n",
        "            previous_task_loader=loadTaskDataset(trainDataset,classGroups[i])\n",
        "            accuracy_after_task=evaluateTaskAccuracy(wpModel,featureExtractor,previous_task_loader,taskNum)\n",
        "            task_accuracies[i].append(accuracy_after_task)\n",
        "            print(f\"Accuracy on task {i+1} after learning task {taskNum+1}: {accuracy_after_task*100:.2f}%\")\n",
        "        if taskNum>0:\n",
        "            forgetting_rates=[]\n",
        "            for i in range(taskNum):\n",
        "                initial_accuracy=task_accuracies[i][0]\n",
        "                current_accuracy=task_accuracies[i][-1]\n",
        "                forgetting=initial_accuracy-current_accuracy\n",
        "                forgetting_rates.append(forgetting)\n",
        "            avg_forgetting_rate.append(sum(forgetting_rates)/len(forgetting_rates))\n",
        "            print(f\"Average Forgetting Rate after task {taskNum+1}: {avg_forgetting_rate[-1]:.4f}\")\n",
        "        buffer.addSamples(XTask,yTask)\n",
        "    aca=sum([task_accuracies[i][-1] for i in range(len(classGroups))])/len(classGroups)\n",
        "    print(f\"\\nAverage Classification Accuracy (ACA) after the last task: {aca:.4f}\")\n",
        "    return aca,avg_forgetting_rate\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "trainDataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "wpModel=MultiTaskCNN()\n",
        "trainIncrementalClassTasks(featureExtractor,wpModel,trainDataset,classGroups)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "def loadTaskTestDataset(testData,taskClasses):\n",
        "    taskIndices=[i for i,(_,label) in enumerate(testData) if label in taskClasses]\n",
        "    taskDataset=torch.utils.data.Subset(testData,taskIndices)\n",
        "    def remapLabels(data):\n",
        "        images,labels=data\n",
        "        labelMap={taskClasses[0]:0,taskClasses[1]:1}\n",
        "        return images,labelMap[labels]\n",
        "    remappedDataset=[(remapLabels(item)) for item in taskDataset]\n",
        "    return DataLoader(remappedDataset,batch_size=64,shuffle=False)\n",
        "\n",
        "def testModel(featureExtractor,wpModel,testDataset,classGroups):\n",
        "    wpModel.eval()\n",
        "    totalCorrect=0\n",
        "    totalSamples=0\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for taskNum,taskClasses in enumerate(classGroups):\n",
        "            print(f\"\\nTesting on task {taskNum+1} with classes: {taskClasses}\")\n",
        "            taskTestLoader=loadTaskTestDataset(testDataset,taskClasses)\n",
        "            taskCorrect=0\n",
        "            taskTotal=0\n",
        "            taskLoss=0\n",
        "            for images,labels in taskTestLoader:\n",
        "                features=featureExtractor(images,taskNum)\n",
        "                outputs=wpModel(features,taskNum)\n",
        "                loss=criterion(outputs,labels)\n",
        "                taskLoss+=loss.item()\n",
        "                _,predicted=torch.max(outputs,1)\n",
        "                taskTotal+=labels.size(0)\n",
        "                taskCorrect+=(predicted==labels).sum().item()\n",
        "            taskAccuracy=100*taskCorrect/taskTotal\n",
        "            avgLoss=taskLoss/len(taskTestLoader)\n",
        "            print(f\"Task {taskNum+1} Accuracy: {taskAccuracy:.2f}%\")\n",
        "            print(f\"Task {taskNum+1} Loss: {avgLoss:.4f}\")\n",
        "            totalCorrect+=taskCorrect\n",
        "            totalSamples+=taskTotal\n",
        "    overallAccuracy=100*totalCorrect/totalSamples\n",
        "    print(f\"\\nOverall Accuracy on all tasks: {overallAccuracy:.2f}%\")\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "testDataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "classGroups=[[0,1],[2,3],[4,5],[6,7],[8,9]]\n",
        "testModel(featureExtractor,wpModel,testDataset,classGroups)"
      ],
      "metadata": {
        "id": "NEszaE0sN27L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}